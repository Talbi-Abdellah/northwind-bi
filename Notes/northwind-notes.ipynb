{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3432a6",
   "metadata": {},
   "source": [
    "livree par client employee et date meme chose avec non livreee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11244d89",
   "metadata": {},
   "source": [
    "# Livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b30626",
   "metadata": {},
   "source": [
    "## Command + client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1035f",
   "metadata": {},
   "source": [
    "# Resultat + employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76941f1",
   "metadata": {},
   "source": [
    "# non Livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff66b70",
   "metadata": {},
   "source": [
    "# model pour represente un entropot projet te3na ndiro etoile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd056f39",
   "metadata": {},
   "source": [
    "Etoile / constelation / flocan de naige \n",
    "Table de fait / Table de dimention\n",
    "1/etile ( une seul table de fait central - pas de relation entre les tables de dimentions mais il ya entre fait et dimention)\n",
    "pas de relation entre les dimentions alors abscence de hierachie entre les dim\n",
    "\n",
    "etoile (simple) ---> flocan (une seul table de fait +dimention seoare+heiarchi entre dim +normalisation) ---> constelation(redondance plusieur fait dim commune entre fait+hearchie entre dimention) ( koul wa7ed rigla lo5er)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdeb6b",
   "metadata": {},
   "source": [
    "mesure = client , ordre , employee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b1119",
   "metadata": {},
   "source": [
    "dim (axe d'analise) = client , ordre , employee,temps (lazem dire dim temps )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466700e",
   "metadata": {},
   "source": [
    "multi dimention = donne de plusieur tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3806472b",
   "metadata": {},
   "source": [
    "Fait te3na hya command 3endha id howa id sequenciel ( num qui incremante) + concatination key primaire des dimentions id1id2...\n",
    "+ attribute te3 command nbr commandes livree/non livree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daead03",
   "metadata": {},
   "source": [
    "# Cleaning ACCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae51dfb",
   "metadata": {},
   "source": [
    "- **Customers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [customers,order_details,orders,products,purchase_orders,shippers,suppliers]:\n",
    "        i.drop_duplicates(inplace=True)\n",
    "\n",
    "#['Order Date'] = i['Order Date'].str.strip()\n",
    "\n",
    "customers['ContactName'] = (\n",
    "    customers['First Name'].fillna('') + ' ' + customers['Last Name'].fillna('')\n",
    ")\n",
    "customers['ContactName'] = customers['ContactName'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "customers.drop(columns=['First Name', 'Last Name'], inplace=True)\n",
    "\n",
    "customers.rename(columns={'Country/Region':'Country'},inplace=True)\n",
    "customers.drop(columns=['E-mail Address','Job Title','Business Phone','Home Phone','Mobile Phone',\n",
    "                        'Fax Number','Address','City','State/Province','ZIP/Postal Code','Web Page',\n",
    "                        'Notes','Attachments'],inplace=True)\n",
    "\n",
    "customers.to_csv('customer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb9f90",
   "metadata": {},
   "source": [
    "- **orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3738c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['Order Date'] = pd.to_datetime(orders['Order Date']).dt.strftime('%Y-%m-%d')\n",
    "orders['Shipped Date'] = pd.to_datetime(orders['Shipped Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Ship Name', 'Ship Address', 'Ship City', 'Ship State/Province',\n",
    "    'Ship ZIP/Postal Code', 'Ship Country/Region',\n",
    "    'Taxes', 'Payment Type', 'Paid Date',\n",
    "    'Notes', 'Tax Rate', 'Tax Status', 'Status ID'\n",
    "]\n",
    "orders.drop(columns=[c for c in cols_to_drop if c in orders.columns], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "orders.columns = orders.columns.str.replace(' ','',regex = False)\n",
    "\n",
    "orders.to_csv('order.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60d4d2",
   "metadata": {},
   "source": [
    "- **Employee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'First Name' in employees.columns or 'Last Name' in employees.columns:\n",
    "    employees['FullName'] = (\n",
    "        employees.get('First Name', '').fillna('') + ' ' + employees.get('Last Name', '').fillna('')\n",
    "    ).str.strip()\n",
    "    # drop the original name columns if they exist\n",
    "    employees.drop(columns=[c for c in ['First Name', 'Last Name'] if c in employees.columns], inplace=True)\n",
    "\n",
    "cols_to_remove = [\n",
    "    'E-mail Address','Job Title','Business Phone','Home Phone','Mobile Phone',\n",
    "    'Fax Number','Address','City','State/Province','ZIP/Postal Code',\n",
    "    'Country/Region','Web Page','Notes','Attachments'\n",
    "]\n",
    "employees.drop(columns=[c for c in cols_to_remove if c in employees.columns], inplace=True)\n",
    "\n",
    "employees.to_csv('employees.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a42b2b",
   "metadata": {},
   "source": [
    "# Cleaning SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e546a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.rename(columns={'CompanyName':'Company'},inplace=True)\n",
    "cols_to_drop = [\n",
    "    'ContactTitle','Address','City','Region','PostalCode','Phone','Fax'\n",
    "]\n",
    "df_customers.drop(columns=[c for c in cols_to_drop if c in df_customers.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf6e3a",
   "metadata": {},
   "source": [
    "# ANALYTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9614fc",
   "metadata": {},
   "source": [
    "## 1- TRANSFORM WAREHOUSE INTO CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eae33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# Encode the ODBC driver name\n",
    "params = urllib.parse.quote_plus(\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "    \"Server=localhost\\\\SQLEXPRESS;\"\n",
    "    \"Database=Northwind_DW;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "\n",
    "dim_customer = pd.read_sql(\"SELECT * FROM DimCustomer\", engine)\n",
    "dim_employee = pd.read_sql(\"SELECT * FROM DimEmployee\", engine)\n",
    "dim_time = pd.read_sql(\"SELECT * FROM DimTime\", engine)\n",
    "order_fact = pd.read_sql(\"SELECT * FROM Order_Fact\", engine)\n",
    "\n",
    "# Export to CSV\n",
    "dim_customer.to_csv(\"../Data/warehouse/DimCustomer.csv\", index=False)\n",
    "dim_employee.to_csv(\"../Data/warehouse/DimEmployee.csv\", index=False)\n",
    "dim_time.to_csv(\"../Data/warehouse/DimTime.csv\", index=False)\n",
    "order_fact.to_csv(\"../Data/warehouse/Order_Fact.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aaf8bc",
   "metadata": {},
   "source": [
    "## 2- TRANSFORM THE WAREHOUSE CSV FILES INTO SPECIFIC ANALYTICS / DASHBOARDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3267055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "customers = pd.read_csv(\"../Data/warehouse/DimCustomer.csv\")\n",
    "employees = pd.read_csv(\"../Data/warehouse/DimEmployee.csv\")\n",
    "orders = pd.read_csv(\"../Data/warehouse/Order_Fact.csv\")\n",
    "time_dim = pd.read_csv(\"../Data/warehouse/DimTime.csv\")\n",
    "\n",
    "customers.columns = customers.columns.str.strip()\n",
    "employees.columns = employees.columns.str.strip()\n",
    "orders.columns = orders.columns.str.strip()\n",
    "time_dim.columns = time_dim.columns.str.strip()\n",
    "\n",
    "orders['ShippedDateKey'] = orders['ShippedDateKey'].astype(int)\n",
    "\n",
    "shipped_summary = pd.DataFrame({\n",
    "    'ShippedStatus': ['Shipped', 'NotShipped'],\n",
    "    'Count': [\n",
    "        (orders['ShippedDateKey'] != 1011900).sum(),\n",
    "        (orders['ShippedDateKey'] == 1011900).sum()\n",
    "    ]\n",
    "})\n",
    "shipped_summary.to_csv(\"../Data/analytics/shipped_summary.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "shipped_by_employee = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_employee = shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "shipped_by_employee.to_csv(\"../Data/analytics/shipped_by_employee.csv\", index=False)\n",
    "\n",
    "\n",
    "shipped_by_customer = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='ShippedCount')\n",
    "shipped_by_customer = shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "shipped_by_customer.to_csv(\"../Data/analytics/shipped_by_customer.csv\", index=False)\n",
    "\n",
    "shipped_by_country = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='ShippedCount')\n",
    "shipped_by_country.to_csv(\"../Data/analytics/shipped_by_country.csv\", index=False)\n",
    "\n",
    "shipped_by_company = orders[orders['ShippedDateKey'] != 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='ShippedCount')\n",
    "shipped_by_company.to_csv(\"../Data/analytics/shipped_by_company.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Not shipped by employee\n",
    "not_shipped_by_employee = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('EmployeeID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_employee = not_shipped_by_employee.merge(employees[['EmployeeID','FullName']], on='EmployeeID')\n",
    "not_shipped_by_employee.to_csv(\"../Data/analytics/not_shipped_by_employee.csv\", index=False)\n",
    "\n",
    "# Not shipped by customer\n",
    "not_shipped_by_customer = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .groupby('CustomerID').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_customer = not_shipped_by_customer.merge(customers[['CustomerID','Company']], on='CustomerID')\n",
    "not_shipped_by_customer.to_csv(\"../Data/analytics/not_shipped_by_customer.csv\", index=False)\n",
    "\n",
    "# Not shipped by country\n",
    "not_shipped_by_country = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Country']], on='CustomerID')\\\n",
    "    .groupby('Country').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_country.to_csv(\"../Data/analytics/not_shipped_by_country.csv\", index=False)\n",
    "\n",
    "# Not shipped by company\n",
    "not_shipped_by_company = orders[orders['ShippedDateKey'] == 1011900]\\\n",
    "    .merge(customers[['CustomerID','Company']], on='CustomerID')\\\n",
    "    .groupby('Company').size().reset_index(name='NotShippedCount')\n",
    "not_shipped_by_company.to_csv(\"../Data/analytics/not_shipped_by_company.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "shipped_orders = orders[orders['ShippedDateKey'] != 1011900]\n",
    "\n",
    "# Merge with DimTime on ShippedDateKey\n",
    "shipped_orders = shipped_orders.merge(\n",
    "    time_dim[['DateKey', 'Year', 'Month']], \n",
    "    left_on='ShippedDateKey', \n",
    "    right_on='DateKey', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "shipped_per_year = shipped_orders.groupby('Year').size().reset_index(name='ShippedCount')\n",
    "shipped_per_year = shipped_per_year.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_year.to_csv(\"../Data/analytics/most_shipped_per_year.csv\", index=False)\n",
    "\n",
    "# Most shipped per month (across all years)\n",
    "shipped_per_month = shipped_orders.groupby('Month').size().reset_index(name='ShippedCount')\n",
    "shipped_per_month = shipped_per_month.sort_values(by='ShippedCount', ascending=False)\n",
    "shipped_per_month.to_csv(\"../Data/analytics/most_shipped_per_month.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
